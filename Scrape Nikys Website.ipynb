{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4f7e35-1cf5-4846-b74f-9124199e5dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for web scraping\n",
    "%pip install requests beautifulsoup4 selenium pandas lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ede15cb-ba8e-451c-ae61-8c98e16bcf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to scrape the website...\n",
      "Page loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Set up Firefox options for headless browsing\n",
    "firefox_options = Options()\n",
    "firefox_options.add_argument(\"--headless\")\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "print(\"Starting to scrape the website...\")\n",
    "\n",
    "# Navigate to the URL\n",
    "url = \"https://www.nikys-sports.com/collections/turf?page=1\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"Page loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f2a4ce-f2b2-4fab-983a-28839862cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to scrape with requests...\n",
      "Status code: 200\n",
      "Content length: 1720352\n",
      "Successfully parsed HTML content\n"
     ]
    }
   ],
   "source": [
    "# Let's try using requests and BeautifulSoup first to see if we can scrape the content\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Try to get the page content with requests first\n",
    "url = \"https://www.nikys-sports.com/collections/turf?page=1\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "print(\"Attempting to scrape with requests...\")\n",
    "response = requests.get(url, headers=headers)\n",
    "print(\"Status code:\", response.status_code)\n",
    "print(\"Content length:\", len(response.content))\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(\"Successfully parsed HTML content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ac77e10-5baa-4a01-a2e3-2a152b085e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 potential product containers\n",
      "Found 3 grid/collection containers\n",
      "Found 0 tables\n",
      "Page title: Turf Soccer Shoes | Niky's Sports\n",
      "Found 2 product links\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the page structure to find product information\n",
    "# Look for product containers, cards, or similar elements\n",
    "\n",
    "# Find all potential product containers\n",
    "product_containers = soup.find_all('div', class_=lambda x: x and ('product' in x.lower() or 'item' in x.lower() or 'card' in x.lower()))\n",
    "print(\"Found\", len(product_containers), \"potential product containers\")\n",
    "\n",
    "# Let's also look for common e-commerce patterns\n",
    "grid_containers = soup.find_all('div', class_=lambda x: x and ('grid' in x.lower() or 'collection' in x.lower()))\n",
    "print(\"Found\", len(grid_containers), \"grid/collection containers\")\n",
    "\n",
    "# Look for any tables\n",
    "tables = soup.find_all('table')\n",
    "print(\"Found\", len(tables), \"tables\")\n",
    "\n",
    "# Let's examine the page title and some key elements\n",
    "title = soup.find('title')\n",
    "if title:\n",
    "    print(\"Page title:\", title.get_text().strip())\n",
    "\n",
    "# Look for product links or anchors\n",
    "product_links = soup.find_all('a', href=lambda x: x and '/products/' in x)\n",
    "print(\"Found\", len(product_links), \"product links\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19582a4e-f5be-4f1b-a337-21c2661a6cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product cards found: 0\n",
      "Price elements found: 7\n",
      "Title elements found: 13\n",
      "Found main content area\n",
      "No collection grid found in main content\n"
     ]
    }
   ],
   "source": [
    "# Let's look for specific product information patterns\n",
    "# First, let's examine the structure more carefully\n",
    "\n",
    "# Look for product cards or items with more specific selectors\n",
    "product_cards = soup.find_all('div', class_=lambda x: x and any(keyword in x.lower() for keyword in ['product-card', 'product-item', 'item-card', 'card-product']))\n",
    "print(\"Product cards found:\", len(product_cards))\n",
    "\n",
    "# Look for price elements\n",
    "prices = soup.find_all(class_=lambda x: x and 'price' in x.lower())\n",
    "print(\"Price elements found:\", len(prices))\n",
    "\n",
    "# Look for product titles/names\n",
    "titles = soup.find_all(class_=lambda x: x and any(keyword in x.lower() for keyword in ['title', 'name', 'product-title']))\n",
    "print(\"Title elements found:\", len(titles))\n",
    "\n",
    "# Let's examine the actual HTML structure by looking at some key divs\n",
    "main_content = soup.find('main') or soup.find('div', {'id': 'main'}) or soup.find('div', class_=lambda x: x and 'main' in x.lower())\n",
    "if main_content:\n",
    "    print(\"Found main content area\")\n",
    "    # Look for collection or product grid within main content\n",
    "    collection_grid = main_content.find('div', class_=lambda x: x and any(keyword in x.lower() for keyword in ['collection', 'grid', 'products']))\n",
    "    if collection_grid:\n",
    "        print(\"Found collection grid within main content\")\n",
    "        print(\"Grid classes:\", collection_grid.get('class', []))\n",
    "    else:\n",
    "        print(\"No collection grid found in main content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5827ccd1-906e-4d5e-999f-e32fd9a1d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 118 script tags\n",
      "Found 4 JSON-LD scripts\n",
      "Found 27 scripts potentially containing product data\n",
      "Found 0 article tags\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(articles), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle tags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Look for any elements with data attributes that might contain product info\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m elements_with_data \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(elements_with_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements with data attributes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:2035\u001b[0m, in \u001b[0;36mTag.find_all\u001b[1;34m(self, name, attrs, recursive, string, limit, **kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m     generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren\n\u001b[0;32m   2034\u001b[0m _stacklevel \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_stacklevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m-> 2035\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_all(name, attrs, string, limit, generator,\n\u001b[0;32m   2036\u001b[0m                       _stacklevel\u001b[38;5;241m=\u001b[39m_stacklevel\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:841\u001b[0m, in \u001b[0;36mPageElement._find_all\u001b[1;34m(self, name, attrs, string, limit, generator, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i:\n\u001b[1;32m--> 841\u001b[0m     found \u001b[38;5;241m=\u001b[39m strainer\u001b[38;5;241m.\u001b[39msearch(i)\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[0;32m    843\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(found)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:2325\u001b[0m, in \u001b[0;36mSoupStrainer.search\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m   2323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, Tag):\n\u001b[0;32m   2324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m-> 2325\u001b[0m         found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_tag(markup)\n\u001b[0;32m   2326\u001b[0m \u001b[38;5;66;03m# If it's text, make sure the text matches.\u001b[39;00m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, NavigableString) \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m   2328\u001b[0m          \u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:2288\u001b[0m, in \u001b[0;36mSoupStrainer.search_tag\u001b[1;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[0;32m   2286\u001b[0m             markup_attr_map[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m   2287\u001b[0m attr_value \u001b[38;5;241m=\u001b[39m markup_attr_map\u001b[38;5;241m.\u001b[39mget(attr)\n\u001b[1;32m-> 2288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_matches(attr_value, match_against):\n\u001b[0;32m   2289\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:2343\u001b[0m, in \u001b[0;36mSoupStrainer._matches\u001b[1;34m(self, markup, match_against, already_tried)\u001b[0m\n\u001b[0;32m   2339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   2340\u001b[0m     \u001b[38;5;66;03m# This should only happen when searching a multi-valued attribute\u001b[39;00m\n\u001b[0;32m   2341\u001b[0m     \u001b[38;5;66;03m# like 'class'.\u001b[39;00m\n\u001b[0;32m   2342\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m markup:\n\u001b[1;32m-> 2343\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_matches(item, match_against):\n\u001b[0;32m   2344\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2345\u001b[0m     \u001b[38;5;66;03m# We didn't match any particular value of the multivalue\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m     \u001b[38;5;66;03m# attribute, but maybe we match the attribute value when\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m     \u001b[38;5;66;03m# considered as a string.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\element.py:2357\u001b[0m, in \u001b[0;36mSoupStrainer._matches\u001b[1;34m(self, markup, match_against, already_tried)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m markup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(match_against, Callable):\n\u001b[1;32m-> 2357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m match_against(markup)\n\u001b[0;32m   2359\u001b[0m \u001b[38;5;66;03m# Custom callables take the tag as an argument, but all\u001b[39;00m\n\u001b[0;32m   2360\u001b[0m \u001b[38;5;66;03m# other ways of matching match the tag name as a string.\u001b[39;00m\n\u001b[0;32m   2361\u001b[0m original_markup \u001b[38;5;241m=\u001b[39m markup\n",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(articles), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle tags\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Look for any elements with data attributes that might contain product info\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m elements_with_data \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(elements_with_data), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements with data attributes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# Let's try a different approach - look for JSON data or script tags that might contain product information\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Look for script tags that might contain product data\n",
    "scripts = soup.find_all('script')\n",
    "print(\"Found\", len(scripts), \"script tags\")\n",
    "\n",
    "# Look for JSON-LD structured data\n",
    "json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "print(\"Found\", len(json_ld_scripts), \"JSON-LD scripts\")\n",
    "\n",
    "# Look for scripts containing product data\n",
    "product_data_scripts = []\n",
    "for script in scripts:\n",
    "    if script.string and any(keyword in script.string.lower() for keyword in ['product', 'collection', 'items', 'variants']):\n",
    "        product_data_scripts.append(script)\n",
    "\n",
    "print(\"Found\", len(product_data_scripts), \"scripts potentially containing product data\")\n",
    "\n",
    "# Let's also try to find the actual product elements by looking at the page structure\n",
    "# Sometimes products are in article tags or specific div structures\n",
    "articles = soup.find_all('article')\n",
    "print(\"Found\", len(articles), \"article tags\")\n",
    "\n",
    "# Look for any elements with data attributes that might contain product info\n",
    "elements_with_data = soup.find_all(attrs=lambda x: x and any(key.startswith('data-') for key in x.keys()))\n",
    "print(\"Found\", len(elements_with_data), \"elements with data attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f4a44-bb49-4423-b255-d4390fb6ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the attribute search and look for data attributes more carefully\n",
    "# Let's examine the JSON-LD scripts first since we found 4 of them\n",
    "json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "\n",
    "print(\"Examining JSON-LD scripts:\")\n",
    "for i, script in enumerate(json_ld_scripts):\n",
    "    if script.string:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            print(\"Script\", i + 1, \"contains:\", type(data))\n",
    "            if isinstance(data, dict):\n",
    "                print(\"Keys:\", list(data.keys())[:5])  # Show first 5 keys\n",
    "            elif isinstance(data, list) and len(data) > 0:\n",
    "                print(\"List with\", len(data), \"items\")\n",
    "                if isinstance(data[0], dict):\n",
    "                    print(\"First item keys:\", list(data[0].keys())[:5])\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Script\", i + 1, \"contains invalid JSON\")\n",
    "\n",
    "# Let's also look for elements with specific data attributes\n",
    "elements_with_product_data = soup.find_all(attrs={'data-product-id': True})\n",
    "print(\"Elements with data-product-id:\", len(elements_with_product_data))\n",
    "\n",
    "elements_with_variant_data = soup.find_all(attrs={'data-variant-id': True})\n",
    "print(\"Elements with data-variant-id:\", len(elements_with_variant_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a2096-d061-4215-b97f-a9ac16720c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the JSON-LD scripts in detail to see if they contain product data\n",
    "json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "\n",
    "for i, script in enumerate(json_ld_scripts):\n",
    "    if script.string:\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "            print(\"=== JSON-LD Script\", i + 1, \"===\")\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                # Check if this contains product information\n",
    "                if data.get('@type') == 'Product':\n",
    "                    print(\"Found Product schema!\")\n",
    "                    print(\"Product data:\", data)\n",
    "                elif data.get('@type') == 'BreadcrumbList':\n",
    "                    print(\"Breadcrumb navigation data\")\n",
    "                    if 'itemListElement' in data:\n",
    "                        print(\"Breadcrumb items:\", len(data['itemListElement']))\n",
    "                elif data.get('@type') == 'Organization':\n",
    "                    print(\"Organization data\")\n",
    "                else:\n",
    "                    print(\"Type:\", data.get('@type'))\n",
    "                    print(\"Keys:\", list(data.keys()))\n",
    "                    # Print first few key-value pairs\n",
    "                    for key in list(data.keys())[:3]:\n",
    "                        print(key + \":\", str(data[key])[:100])\n",
    "            print()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Script\", i + 1, \"JSON decode error:\", e)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf7a5c8-0f84-451a-9f24-c8efd0512c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ItemList contains 30 products\n",
      "\n",
      "=== Product 1 ===\n",
      "@type: ListItem\n",
      "position: 1\n",
      "url: https://www.nikys-sports.com/products/f50-league-tf-j\n",
      "name: adidas F50 League Kids Turf Soccer Shoes\n",
      "\n",
      "=== Product 2 ===\n",
      "@type: ListItem\n",
      "position: 2\n",
      "url: https://www.nikys-sports.com/products/nike-mercurial-superfly-10-tf-2\n",
      "name: Nike Mercurial Superfly 10 Academy Turf High-Top Soccer Shoes\n",
      "\n",
      "=== Product 3 ===\n",
      "@type: ListItem\n",
      "position: 3\n",
      "url: https://www.nikys-sports.com/products/f50-elite-tf\n",
      "name: adidas F50 Pro Turf Soccer Shoes\n",
      "\n",
      "Available keys across all products: ['@type', 'name', 'position', 'url']\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the ItemList JSON-LD script in detail - this likely contains our product data\n",
    "json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "\n",
    "# Get the ItemList script (script 3)\n",
    "itemlist_script = json_ld_scripts[2]  # 0-indexed, so script 3 is index 2\n",
    "data = json.loads(itemlist_script.string)\n",
    "\n",
    "print(\"ItemList contains\", len(data['itemListElement']), \"products\")\n",
    "print()\n",
    "\n",
    "# Let's examine the first few products\n",
    "for i, item in enumerate(data['itemListElement'][:3]):\n",
    "    print(\"=== Product\", i + 1, \"===\")\n",
    "    for key, value in item.items():\n",
    "        print(key + \":\", value)\n",
    "    print()\n",
    "\n",
    "# Let's also see what keys are available in all products\n",
    "all_keys = set()\n",
    "for item in data['itemListElement']:\n",
    "    all_keys.update(item.keys())\n",
    "\n",
    "print(\"Available keys across all products:\", sorted(all_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1afa9f-7ad5-48b6-ba92-a5e022f5f869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 30 products from the turf category\n",
      "\n",
      "Sample of the data:\n",
      "  position                                               name  \\\n",
      "0        1           adidas F50 League Kids Turf Soccer Shoes   \n",
      "1        2  Nike Mercurial Superfly 10 Academy Turf High-T...   \n",
      "2        3                   adidas F50 Pro Turf Soccer Shoes   \n",
      "3        4                       adidas F50 League Turf Shoes   \n",
      "4        5        New Balance Furon Elite Turf V8 Soccer Shoe   \n",
      "5        6  Nike Tiempo Legend 10 Academy Turf Low-Top Soc...   \n",
      "6        7  Nike Mercurial Vapor 16 Academy TURF Low-Top S...   \n",
      "7        8  Nike Mercurial Vapor 16 Academy &quot;Kylian M...   \n",
      "8        9                   adidas F50 Pro Turf Soccer Shoes   \n",
      "9       10              adidas Predator Pro Turf Soccer Shoes   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.nikys-sports.com/products/f50-leag...  \n",
      "1  https://www.nikys-sports.com/products/nike-mer...  \n",
      "2  https://www.nikys-sports.com/products/f50-elit...  \n",
      "3  https://www.nikys-sports.com/products/f50-leag...  \n",
      "4  https://www.nikys-sports.com/products/furon-pr...  \n",
      "5  https://www.nikys-sports.com/products/tiempo-l...  \n",
      "6  https://www.nikys-sports.com/products/nike-mer...  \n",
      "7  https://www.nikys-sports.com/products/nike-mer...  \n",
      "8  https://www.nikys-sports.com/products/f50-pro-...  \n",
      "9  https://www.nikys-sports.com/products/predator...  \n",
      "\n",
      "Data saved to nikys_sports_turf_products.csv\n",
      "Full dataset shape: (30, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract all product data from the JSON-LD ItemList\n",
    "import pandas as pd\n",
    "\n",
    "json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "itemlist_script = json_ld_scripts[2]\n",
    "data = json.loads(itemlist_script.string)\n",
    "\n",
    "# Create a list to store product information\n",
    "products = []\n",
    "\n",
    "for item in data['itemListElement']:\n",
    "    product = {\n",
    "        'position': item.get('position'),\n",
    "        'name': item.get('name'),\n",
    "        'url': item.get('url')\n",
    "    }\n",
    "    products.append(product)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "print(\"Successfully extracted\", len(products), \"products from the turf category\")\n",
    "print()\n",
    "print(\"Sample of the data:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Save to CSV file\n",
    "df.to_csv('nikys_sports_turf_products.csv', index=False)\n",
    "print()\n",
    "print(\"Data saved to nikys_sports_turf_products.csv\")\n",
    "print(\"Full dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7ca0b95-598b-4147-b2dc-8039ef54c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 30 products from the turf category\n",
      "\n",
      "Sample of the data:\n",
      "  position                                               name  \\\n",
      "0        1           adidas F50 League Kids Turf Soccer Shoes   \n",
      "1        2  Nike Mercurial Superfly 10 Academy Turf High-T...   \n",
      "2        3                   adidas F50 Pro Turf Soccer Shoes   \n",
      "3        4                       adidas F50 League Turf Shoes   \n",
      "4        5        New Balance Furon Elite Turf V8 Soccer Shoe   \n",
      "5        6  Nike Tiempo Legend 10 Academy Turf Low-Top Soc...   \n",
      "6        7  Nike Mercurial Vapor 16 Academy TURF Low-Top S...   \n",
      "7        8  Nike Mercurial Vapor 16 Academy &quot;Kylian M...   \n",
      "8        9                   adidas F50 Pro Turf Soccer Shoes   \n",
      "9       10              adidas Predator Pro Turf Soccer Shoes   \n",
      "\n",
      "                                                 url  \n",
      "0  https://www.nikys-sports.com/products/f50-leag...  \n",
      "1  https://www.nikys-sports.com/products/nike-mer...  \n",
      "2  https://www.nikys-sports.com/products/f50-elit...  \n",
      "3  https://www.nikys-sports.com/products/f50-leag...  \n",
      "4  https://www.nikys-sports.com/products/furon-pr...  \n",
      "5  https://www.nikys-sports.com/products/tiempo-l...  \n",
      "6  https://www.nikys-sports.com/products/nike-mer...  \n",
      "7  https://www.nikys-sports.com/products/nike-mer...  \n",
      "8  https://www.nikys-sports.com/products/f50-pro-...  \n",
      "9  https://www.nikys-sports.com/products/predator...  \n",
      "\n",
      "Data saved to nikys_sports_turf_products.csv\n",
      "Full dataset shape: (30, 3)\n"
     ]
    }
   ],
   "source": [
    "# Extract all product data from the JSON-LD ItemList\n",
    "import pandas as pd\n",
    "\n",
    "json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})\n",
    "itemlist_script = json_ld_scripts[2]\n",
    "data = json.loads(itemlist_script.string)\n",
    "\n",
    "# Create a list to store product information\n",
    "products = []\n",
    "\n",
    "for item in data['itemListElement']:\n",
    "    product = {\n",
    "        'position': item.get('position'),\n",
    "        'name': item.get('name'),\n",
    "        'url': item.get('url')\n",
    "    }\n",
    "    products.append(product)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(products)\n",
    "\n",
    "print(\"Successfully extracted\", len(products), \"products from the turf category\")\n",
    "print()\n",
    "print(\"Sample of the data:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Save to CSV file\n",
    "df.to_csv('nikys_sports_turf_products.csv', index=False)\n",
    "print()\n",
    "print(\"Data saved to nikys_sports_turf_products.csv\")\n",
    "print(\"Full dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e710d9-1be7-4604-8f39-322ce8bba752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped data from the website (downloadable link)\n",
    "http://localhost:8889/files/nikys_sports_turf_products.csv?_xsrf=2%7C4053be09%7C1b7ddba357cff65c167c7be4e086f2c5%7C1749648802"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3] *",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
